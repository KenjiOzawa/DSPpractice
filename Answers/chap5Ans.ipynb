{"nbformat":4,"nbformat_minor":0,"metadata":{"anaconda-cloud":{},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.6"},"colab":{"provenance":[],"toc_visible":true}},"cells":[{"cell_type":"markdown","metadata":{"id":"EJXfdMUqc2oB"},"source":["# 第5章　音のフィルタリング（確認問題の解答）\n","コードのセルを，本文ノートブック Chap5.ipynb にコピペして実行してください。"]},{"cell_type":"markdown","metadata":{"id":"6Kr6vKUIc2oC"},"source":["## 5.1 FIR フィルタによる雑音の除去 \n","### (a) インパルス応答の畳み込み \n"]},{"cell_type":"markdown","metadata":{"id":"k5p0ZVDgc2oD"},"source":["(1) 上記 (b) では，雑音除去のために LPF を用いた。もしプログラム中の\n","\n","wave_data += noise.astype(int)                     # 雑音を重畳\n","\n","の行をコメントアウトすれば，「音楽の高域をカットすることによる音質の変化」を観測することができる。カットオフ周波数を様々に変えて，音質の変化を観測しなさい。（ヒント： fc = 8e3 とすればラジオの AM 放送なみ，fc = 3.4e3 とすれば電話音声なみの音質になる）。\n","\n","【答】電話音声帯域では「音楽を伝送するのには不十分」と感じていただけたでしょうか？"]},{"cell_type":"markdown","metadata":{"id":"IEmL3_qEc2oD"},"source":["(2) 上記 (b) では，h_final （円状シフトした後，窓掛けしたインパルス応答）を用いて LPF をかけた。これを，h （逆 FFT しただけ）や，h_shifted （円状シフトしただけ）を用いた場合に，h_final を用いた場合と違いがあるか，確認しなさい。\n","\n","【答】元のノートブックからインプット5.4～5.8をコピーして，整理したコードを以下に示します。本質的な変更はインプット5.8の部分における，h_final を h_shifted や h に置換した部分だけです。"]},{"cell_type":"code","metadata":{"id":"QqS9Wx-sc2oE"},"source":["# インプット5.4\n","# (1) 遮断周波数を決め，その周波数未満は振幅 1 ，それ以上は振幅 0 のペクトルを用意\n","\n","fs = 44.1e3  # 標本化周波数は 44.1 kHz\n","N = 64      # タップ長を設定\n","fc = 6e3    # 遮断周波数（カットオフ周波数）を設定\n","\n","kc = int(np.floor(fc / fs * N ))  # 遮断周波数に対応する，周波数ビンを算出\n","H = np.zeros(N)                   # まずは，全周波数ビンの振幅を 0 として初期化\n","H[0: kc] = 1.0                    # 遮断周波数未満は振幅を 1 に設定\n","H[-kc+1: ] = 1.0                  # 負の周波数領域における振幅値も 1 に設定\n","\n","# インプット5.5\n","# (2) 逆 FFT することで，インパルス応答を算出\n","h = np.real( np.fft.ifft(H) )  # 逆 FFT して，虚部のゴミを無視\n","\n","# インプット 5.6\n","# (3-A) 円状シフトを実施\n","h_shifted = circle_shift(h, int(N/2))\n","\n","# インプット 5.7 \n","fs, wave_data_original = scipy.io.wavfile.read ('sample/sample3.wav')\n","sampling_interval = 1.0 / fs\n","times = np.arange ( len ( wave_data_original )) * sampling_interval\n","wave_data = wave_data_original.copy()\n","\n","mod = 2e3*np.cos(2*np.pi*0.25*times)\n","noise = 500.0 * np.sin(2*np.pi*8e3*times + mod)\n","wave_data += noise.astype(int)                     # 雑音を重畳します。\n","\n","# インプット 5.8\n","# 型を float に変換したうえで，畳み込み\n","result = np.convolve(h_shifted, wave_data.astype(float))   # h_final を h_shifted と置換\n","#result = np.convolve(h, wave_data.astype(float))         # h_final を h と置換\n","\n","times = np.arange ( len ( result )) * sampling_interval\n","plt.title(\"Output 5.8\")\n","plot_wave ( times, result )                          # 波形全体を表示\n","plot_wave ( times[0:1000], result[0:1000] )          # 波形の先頭1000点部分のみ表示\n","\n","plt.specgram(result[:], NFFT=256, Fs=fs, cmap='jet')\n","# モノクロプリンタ用には，jet → gray などにしてみるとよい\n","plt.ylabel('Frequency (Hz)')\n","plt.xlabel('Time (s)')\n","plt.show()\n","\n","audio = Audio(result[:], rate = fs)\n","audio"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QE3WJ2Ewc2oJ"},"source":["まず，h_final の代わりに h_shifted を用いると，スペクトログラムの色の変化が急峻であることから，遮断特性は急峻であることが分かります。ただし，波形の先頭部分1000ポイントを拡大して表示すると，入力音の立ち上がりを反映した「暴れ」があることが分かります（同じことを h_final を使った場合についても表示してみれば，違いはかなり大きいことが分かります）。\n","\n","次に，上記のコードで，インプット5.8 の部分における 「# h_final を h と置換」という注釈がついた行のコメントを外してから再度実行してください。h_final の代わりに h を用いると，LPFとしての機能が弱いことが分かります。円状畳み込みの世界で設計したフィルタを，直線畳み込みしているので，うまくいかないのは当然といえるでしょう。"]},{"cell_type":"markdown","metadata":{"id":"pzmjECQXc2oJ"},"source":["(3)  [scipy.signal](https://docs.scipy.org/doc/scipy/reference/signal.html)  において，Filter design のうち [firwin の説明](https://docs.scipy.org/doc/scipy/reference/generated/scipy.signal.firwin.html#scipy.signal.firwin)  を読み，5 kHz 以下をカットする HPF を設計し，フィルタリングしてみなさい。（ヒント：firwin の説明を完璧に理解できなくても OK です。説明の下の Examples があり，その中の「High-pass (‘stop’ from 0 to f):」と書いてある部分を読めば見当がつきます。ただし，フィルタ長は奇数にする必要があります。）\n","\n","【答】 インプット5.9と5.10のうち，フィルタ設計の行に  pass_zero=False を追加するだけです。N = 63としました。"]},{"cell_type":"code","metadata":{"id":"FGsic1Cpc2oK"},"source":["# インプット 5.9\n","from scipy import signal\n","N = 63\n","fc = 5e3 / (fs / 2.0) # 遮断周波数は，ナイキスト周波数 (fs/2) に対する比で指定 \n","h = signal.firwin(N, fc, pass_zero=False, window='hann') # ここで HPF の設計\n","plt.title(\"Output 5.9\")\n","plot_wave([], h)\n","\n","# インプット 5.10\n","fs, wave_data_original = scipy.io.wavfile.read ('sample/sample3.wav')\n","print('Sampling frequency =', fs, '[Hz]')\n","sampling_interval = 1.0 / fs\n","times = np.arange ( len ( wave_data )) * sampling_interval\n","wave_data = wave_data_original.copy()\n","\n","mod = 2e3*np.cos(2*np.pi*0.25*times)\n","noise = 500.0 * np.sin(2*np.pi*8e3*times + mod)\n","wave_data += noise.astype(int)                     # 雑音を重畳\n","\n","result = np.convolve(h, wave_data.astype(float)) # 型を float に変換して畳み込み\n","\n","times = np.arange ( len ( result )) * sampling_interval\n","plt.title(\"Output 5.10\")\n","plot_wave ( times, result )\n","\n","plt.specgram(result[:], NFFT=256, Fs=fs, cmap='jet')\n","plt.ylabel('Frequency (Hz)')\n","plt.xlabel('Time (s)')\n","plt.show()\n","\n","audio = Audio(result[:], rate = fs)\n","audio"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"coy8bvl2c2oN"},"source":["実行結果を聴取すると「音楽のうちの高周波成分」と「周波数変動する雑音」のみが聞こえます。"]},{"cell_type":"markdown","metadata":{"id":"f1uDCAIMc2oO"},"source":["## 5.2 IIR フィルタによる雑音の除去\n","(1) 上記 (a) では，$N=16$ の IIR フィルタを設計した。それと同様な遮断特性を得るためには，FIR フィルタでは次数（タップ長）$N$ をどの程度にする必要があるか，試行錯誤的に確認しなさい。\n","\n","【答】試行錯誤しましょう。まず，$N=16$ の IIR フィルタの遮断特性を確認します。ただし，遮断特性を詳しく見るために，インパルス応答を128点計算して，さらに振幅特性の縦軸を dB 表示にします。また，位相特性は見ないこととします。"]},{"cell_type":"code","metadata":{"id":"lr31Xc-Rc2oO"},"source":["# インプット 5.11\n","from scipy import signal\n","\n","N = 16                          # IIR の次数は 16\n","fc = 6e3 / (fs / 2.0)                             \n","# 遮断周波数は，ナイキスト周波数 (fs/2) に対する比として指定\n","b, a = signal.iirfilter(N, fc, btype='lowpass')   \n","# これで設計（係数 b(k), a(k) の計算）を完了\n","\n","N_test = 128                    # インパルス応答を 128 点分計算\n","unit_pulse = np.append(np.array([1.0]), \\\n","                       np.array([0.0 for i in range(N_test - 1)]))    \n","y = signal.lfilter(b, a, unit_pulse)\n","\n","plt.subplot(3,1,1)\n","plt.title(\"Output 5.11\")\n","plot_wave([],y)\n","\n","sp = np.fft.fft(y)\n","plt.subplot(3,1,2)\n","draw_FFT_spectrum(sp, fs, level=True, draw_range=50.0, phase_spectrum=False)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"grfRQBTKc2oQ"},"source":["続いて，FIRフィルタの次数を変えながら特性を見てみましょう。"]},{"cell_type":"code","metadata":{"id":"0v_EZavrc2oR"},"source":["# インプット 5.9\n","from scipy import signal\n","fc = 6e3 / (fs / 2.0) # 遮断周波数は，ナイキスト周波数 (fs/2) に対する比で指定 \n","\n","N_candidate = [2**i for i in range(4, 8)]\n","\n","for N in N_candidate:\n","    h = signal.firwin(N, fc, window='hann') # 窓関数は「ハニング窓」を利用\n","    plt.subplot(3,1,1)\n","    plt.title(\"N = \" + str(N))\n","    plot_wave([], h)\n","    result = np.convolve(h, unit_pulse)[0:N_test] # 単位パルスに畳み込み，最初のN_test点だけを取り出す\n","    sp = np.fft.fft(result)\n","    plt.subplot(3,1,2)    \n","    draw_FFT_spectrum(sp, fs, level=True, draw_range=50.0, phase_spectrum=False)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xrAk244Jc2oT"},"source":["FIRの次数16では，明らかに遮断特性が劣ります。64次くらいで，ほぼ同等でしょうか？これは，IIR（16次）のインパルス応答が，ほぼ70点くらいで収束することと対応しています。"]},{"cell_type":"markdown","metadata":{"id":"ZGwZKUwHc2oU"},"source":["(2)  [scipy.signal](https://docs.scipy.org/doc/scipy/reference/signal.html) において，Filter design のうち [iirfilter の説明](https://docs.scipy.org/doc/scipy/reference/generated/scipy.signal.iirfilter.html#scipy.signal.iirfilter)を読み，7 kHz ～ 9 kHz の帯域のみを通過させる BPF を設計し，雑音成分だけを取りだしなさい。（ヒント：例題にあるように [低域側遮断周波数, 高域側遮断周波数] のリストを渡すだけです。遮断周波数は，ナイキスト周波数に対する比として与えることに注意してください。）\n","\n","【答】以下のとおりです。"]},{"cell_type":"code","metadata":{"id":"x7v1WDjyc2oU"},"source":["# インプット 5.11\n","from scipy import signal\n","\n","N = 13                          # IIR の次数は 13\n","fc = 6e3 / (fs / 2.0)                             \n","fc_low  = 7e3 / (fs / 2.0)                             \n","fc_high = 9e3 / (fs / 2.0)     \n","# 遮断周波数は，ナイキスト周波数 (fs/2) に対する比として指定\n","b, a = signal.iirfilter(N, [fc_low, fc_high], btype='bandpass')  # これで設計（係数 b(k), a(k) の計算）を完了\n","\n","# インプット 5.12\n","fs, wave_data_original = scipy.io.wavfile.read ('sample/sample3.wav')\n","print('Sampling frequency =', fs, '[Hz]')\n","sampling_interval = 1.0 / fs\n","times = np.arange ( len ( wave_data )) * sampling_interval\n","wave_data =  wave_data_original.copy()\n","\n","mod = 2e3*np.cos(2*np.pi*0.25*times)\n","noise = 500.0 * np.sin(2*np.pi*8e3*times + mod)\n","wave_data += noise.astype(int)                     # 雑音を重畳\n","\n","result = signal.lfilter(b, a, wave_data)\n","\n","times = np.arange ( len ( result )) * sampling_interval\n","plt.title(\"Output 5.12\")\n","plot_wave ( times, result )\n","\n","plt.specgram(result[:], NFFT=256, Fs=fs, cmap='jet')\n","plt.ylabel('Frequency (Hz)')\n","plt.xlabel('Time (s)')\n","plt.show()\n","\n","audio = Audio(result[:], rate = fs)\n","audio"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lqzkStxzc2oY"},"source":["## 5.3 頭部伝達関数を用いた音像定位の制御\n","以下の課題を通じて，2次元配列 (ndarray) やイテレータの使い方に習熟すること。\n","\n","(1) 上記 (b) のプログラムをコピーして，別のモノラルの音楽.wavファイル（例えば TopOfTheWorld.wav）を読み込み，「右45°のスピーカからの音楽を聴取する状況」をヘッドホンで実現しなさい。HRIR データとしては，L0e315a.dat と R0e315a.dat を利用すること。\n","\n","(2) 元のプログラム，および (1) で作成したプログラムをマージして，「左45°と右45°から違う音楽が聞こえてくる状況」をヘッドホンで実現しなさい。2つの音の長さが違う場合にも対処できるようにすること。"]},{"cell_type":"markdown","metadata":{"id":"09qfVFiEc2oZ"},"source":["【答】課題 (1) の前に，まず復習です。単にインプット 5.13 ～ 5.15 をコピーして，整理したコードを以下に示します。実行して，左側から聞こえる音を作ります。"]},{"cell_type":"code","metadata":{"id":"dd9NDs1Jc2oZ"},"source":["#from read_text_data import * # テキストデータから数値を読み込む関数の読み込み\n","\n","# インプット 5.13\n","import sys\n","import struct\n","fs = 44100.0\n","sampling_interval = 1.0 / fs\n","LR = {\"Left\":0, \"Right\":1}               # ディクショナリLR を定義し，左と右に 0 と 1 を割り当て，配列のインデックスとします。\n","\n","hrir_L = read_text_data('sample/L0e045a.dat')   # データ長が不明なので，二つの配列に分けて読み込みます。\n","hrir_R = read_text_data('sample/R0e045a.dat')   # もしデータ長が既知ならば，はじめから2次元配列に読み込むべきでしょうね。\n","\n","hrir_len = min(len(hrir_L), len(hrir_R))\n","hrir = np.zeros((2, hrir_len))           # 両耳のデータを納める2次元配列を用意します。\n","hrir[LR[\"Left\"]]  = hrir_L[0: hrir_len]\n","hrir[LR[\"Right\"]] = hrir_R[0: hrir_len]\n","\n","times = np.arange(hrir_len) * sampling_interval\n","\n","print(\"=== Output 5.13 ===\")\n","for lr in LR:                            # ディクショナリもイテレータです。\n","    plt.subplot(2, 1, LR[lr]+1)\n","    plt.title(\"HRIR (\" + lr + \" ear)\")\n","    plot_wave(times, hrir[LR[lr]]);      # HRIR の波形を描画する．\n","\n","# インプット 5.14\n","fs, wave_data = scipy.io.wavfile.read ('sample/sample.wav')\n","print('Sampling frequency =', fs, '[Hz]')\n","sampling_interval = 1.0 / fs\n","times = np.arange(len(wave_data)) * sampling_interval\n","\n","# インプット 5.15\n","result = np.zeros((2,len(wave_data) + hrir_len - 1))   \n","# N点の信号とM点の信号を畳み込んだ結果は N+M-1 点\n","times = np.arange(result.shape[1]) * sampling_interval \n","# 2次元配列 a の列の数は，a.shape[1] で求められます。 \n","\n","print(\"=== Output 5.15 ===\")\n","for lr in LR:\n","    result[LR[lr]] = np.convolve(hrir[LR[lr]], wave_data.astype(float))\n","    plt.title(\"Convolved music (\" + lr + \" ear)\")\n","    plot_wave( times, result[LR[lr]] )\n","\n","audio = Audio(result, rate = fs)\n","audio"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nFdmQpzJc2op"},"source":["続いて，課題(1)に対応するために，上記のコードにおいて読み込む HRTF データと音楽データを変更したコードを以下に示します。ただし，課題(2) で再利用するために，出来上がりの音の配列名に2をつけておきます。"]},{"cell_type":"code","metadata":{"id":"fwr_J4K8c2oq"},"source":["# インプット 5.13\n","import sys\n","import struct\n","fs = 44100.0\n","sampling_interval = 1.0 / fs\n","LR = {\"Left\":0, \"Right\":1}               # ディクショナリLR を定義し，左と右に 0 と 1 を割り当て，配列のインデックスとします。\n","\n","hrir_L = read_text_data('sample/L0e315a.dat')   # データ長が不明なので，二つの配列に分けて読み込みます。\n","hrir_R = read_text_data('sample/R0e315a.dat')   # もしデータ長が既知ならば，はじめから2次元配列に読み込むべきでしょうね。\n","\n","hrir_len = min(len(hrir_L), len(hrir_R))\n","hrir = np.zeros((2, hrir_len))           # 両耳のデータを納める2次元配列を用意します。\n","hrir[LR[\"Left\"]]  = hrir_L[0: hrir_len]\n","hrir[LR[\"Right\"]] = hrir_R[0: hrir_len]\n","\n","times = np.arange(hrir_len) * sampling_interval\n","\n","print(\"=== Output 5.13 ===\")\n","for lr in LR:                            # ディクショナリもイテレータです。\n","    plt.subplot(2, 1, LR[lr]+1)\n","    plt.title(\"HRIR (\" + lr + \" ear)\")\n","    plot_wave(times, hrir[LR[lr]]);      # HRIR の波形を描画する．\n","\n","# インプット 5.14\n","fs, wave_data = scipy.io.wavfile.read ('sample/sample2.wav')\n","print('Sampling frequency =', fs, '[Hz]')\n","sampling_interval = 1.0 / fs\n","times = np.arange(len(wave_data)) * sampling_interval\n","\n","# インプット 5.15\n","result2 = np.zeros((2,len(wave_data) + hrir_len - 1))   \n","# N点の信号とM点の信号を畳み込んだ結果は N+M-1 点\n","times = np.arange(result2.shape[1]) * sampling_interval \n","# 2次元配列 a の列の数は，a.shape[1] で求められます。 \n","\n","print(\"=== Output 5.15 ===\")\n","for lr in LR:\n","    result2[LR[lr]] = np.convolve(hrir[LR[lr]], wave_data.astype(float))\n","    plt.title(\"Convolved music (\" + lr + \" ear)\")\n","    plot_wave( times, result2[LR[lr]] )\n","\n","audio = Audio(result2, rate = fs)\n","audio"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YrX3XPLWc2o0"},"source":["課題(2) は，これまでに作った2つの音 result と result2 を左右それぞれで重ねる（足し合わせる）だけです。ただし，2つの音の継続時間が異なることには対処が必要です。ここでは，短い方の音に「無音区間」を付加することで長さを揃えることにします。"]},{"cell_type":"code","metadata":{"id":"5caglKXqc2o1"},"source":["# 「無音区間」は np.zeros で作ります。2つの2次元配列は np.concatenate でも結合できますが，ここでは hstack を使ってみます。\n","if result.shape[1] >= result2.shape[1]:\n","    result2 = np.hstack([result2, np.zeros((2, result.shape[1] - result2.shape[1]))])\n","else:\n","    result  = np.hstack([result,  np.zeros((2, result2.shape[1] - result.shape[1]))])\n","\n","final = result + result2\n","audio = Audio(final, rate = fs)\n","audio"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VAFhUaGlc2pA"},"source":["## 5.4 FFT を利用した長い音のフィルタリング\n","\n","(1) 5.3 節 (b) において実施した「HRTF の畳み込みによる音像定位」は，音楽ファイル全体に対する畳み込みをするものであった。これを，5.4 節 (c) の手順により，ブロック単位でバッファしては FFT を用いて処理する方式に書き換えなさい。\n","\n","【答】インプット5.21を書き替えてみました。要点は，「モノラルの場合には，appendで伸ばしていく（オーバーラップアドしていく）」のノリでよいのですが，「ステレオの場合には，片チャネルの配列だけを伸ばすことはできない」です。そこで，畳み込み結果を納める2次元h配列を予め準備しておき，「そこから 2N 点を取り出し，N点だけオーバーラップアドして書き戻す」こととしました。"]},{"cell_type":"code","metadata":{"id":"rgltWE5Rc2pA"},"source":["# インプット 5.21\n","fs = 44.1e3\n","\n","# --- フィルタの準備 ＝ HRIRの読み込み（インプット 5.14からコピー）---\n","fs = 44100.0\n","sampling_interval = 1.0 / fs\n","LR = {\"Left\":0, \"Right\":1}               # ディクショナリLR を定義し，左と右に 0 と 1 を割り当て，配列のインデックスとします。\n","\n","hrir_L = read_text_data('sample/L0e045a.dat')   # データ長が不明なので，二つの配列に分けて読み込みます。\n","hrir_R = read_text_data('sample/R0e045a.dat')   # もしデータ長が既知ならば，はじめから2次元配列に読み込むべきでしょうね。\n","\n","hrir_len = min(len(hrir_L), len(hrir_R)) # 左右の長い方を採用します。\n","N = 2**(int(np.log2(hrir_len)) +1 )      # それよりも長い 2 のベキ乗の値を処理の長さとします。\n","\n","hrir = np.zeros((2, 2 * N))              # 両耳のデータを納める2次元配列を用意します。\n","hrir[LR[\"Left\"], 0: hrir_len]  = hrir_L[0: hrir_len]\n","hrir[LR[\"Right\"], 0: hrir_len] = hrir_R[0: hrir_len]\n","\n","H = np.zeros((2, 2*N), dtype=complex)    # HRTF を納める2次元配列を準備し，\n","for lr in LR:\n","    H[LR[lr]] = np.fft.fft(hrir[LR[lr]])\n","\n","# --- 音の準備 ---\n","fs, x_original = scipy.io.wavfile.read ('sample/sample3.wav')\n","print('Sampling frequency =', fs, '[Hz]')\n","sampling_interval = 1.0 / fs\n","x = x_original.copy()\n","\n","\n","# --- 処理の開始 ---\n","n_loop = int(np.floor(len(x) / N))  # 最後の区間長に満たない点は，処理対象から除外\n","\n","start = 0                                 # 切り出し区間の開始点を初期化\n","y = np.zeros((2, len(x_original)+ 2*N))   # 処理結果を納める配列を初期化 \n","for i in range(0, n_loop):\n","    buf = x[start : start + N]            # start からバッファの長さだけ切り出し\n","    buf = np.append(buf, np.zeros(N))     # 末尾に 0 を追加して (長さ: 2N)\n","    X   = np.fft.fft(buf)                 # FFT して\n","    for lr in LR:\n","        Y = H[LR[lr]] * X                 # スペクトルのかけ算して\n","        buf = np.real(np.fft.ifft(Y))     # 逆 FFT して\n","        y[LR[lr], start : start+2*N+N]   = overlap_add(y[LR[lr], start : start + 2*N], buf, N) \n","        # この行：オーバーラップしながら書き戻す（長さがNだけ延びている）\n","    start += N                            # 次のバッファ区間の先頭に移動\n","\n","times = np.arange( y.shape[1] ) * sampling_interval\n","for lr in LR:\n","    plt.title(\"Convolved music (\" + lr + \" ear)\")\n","    plot_wave( times, y[LR[lr]] )\n","\n","\n","audio = Audio(y, rate = fs)\n","audio"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DkC0nKx0c2pL"},"source":["(2) 5.4 節 (c) のバッファリングは，方形波窓の切り出しと同様な方式である。これを，窓長の半分ずつ推移するハニング窓を使ったバッファリングに書き直しなさい。（前章でみたとおり，「窓長の半分ずつ推移するハニング窓」は，一旦はある時刻の音を 2 つのブロックに分けるが，加算すれば元の瞬時値を復元できるので，5.4 節 (c) で処理したのと同じ出力を得ることができるはずである。）\n","\n","【答】インプット5.21を書き替えてみました。要点は，切り出しの推移を窓長の半分ずつずらすので，オーバーラップアドするときにも，その分だけオーバーラップを多くすることがあることでしょう。"]},{"cell_type":"code","metadata":{"id":"lQ2VKVmJc2pM"},"source":["# インプット 5.21\n","fs = 44.1e3\n","N = 1024               # フィルタのタップ長 と バッファ長\n","fc = 6e3 / (fs / 2.0)  # 遮断周波数 6 kHzを，ナイキスト周波数 (fs/2) に対する比で表現 \n","\n","# --- フィルタの準備 ---\n","h = signal.firwin(N, fc, window='hann') # LPFの設計\n","plt.title(\"Output 5.21\")\n","plot_wave([], h)\n","H = np.fft.fft(np.append(h, np.zeros(N)))\n","\n","# --- 音の準備 ---\n","fs, x_original = scipy.io.wavfile.read ('sample/sample3.wav')\n","print('Sampling frequency =', fs, '[Hz]')\n","sampling_interval = 1.0 / fs\n","times = np.arange ( len ( x_original )) * sampling_interval\n","x = x_original.copy()\n","\n","mod = 2e3*np.cos(2*np.pi*0.25*times)\n","noise = 500.0 * np.sin(2*np.pi*8e3*times + mod)\n","x += noise.astype(int)                           # 雑音を重畳\n","\n","# --- 処理の開始 ---\n","shift = int(N/2)                         # ハニング窓のシフトは N/2\n","n_loop = int(np.floor(len(x) / shift))-1 # 最後の区間長に満たない点は，処理対象から除外\n","\n","start = 0                                 # 切り出し区間の開始点を初期化\n","y = np.array([])                          # 処理結果を納める配列を初期化 \n","for i in range(0, n_loop):\n","    buf = hanning(N) * x[start : start + N]  # start からバッファの長さだけ切り出し，窓掛け\n","    buf = np.append(buf, np.zeros(N))     # 末尾に 0 を追加して\n","    X   = np.fft.fft(buf)                 # FFT して\n","    Y   = H * X\n","    buf = np.real(np.fft.ifft(Y))         # 逆 FFT して \n","    y   = overlap_add(y, buf, N+shift)   # 結果をつなげる\n","    start += shift                       # 次のバッファ区間の先頭に移動\n","\n","times = np.arange ( len( y )) * sampling_interval\n","plot_wave ( times, y )\n","\n","plt.specgram(y, NFFT=2408, Fs=fs, cmap='jet')\n","plt.ylabel('Frequency (Hz)')\n","plt.xlabel('Time (s)')\n","plt.show()\n","\n","audio = Audio(y, rate = fs)\n","audio"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BOoVtcIfc2pU"},"source":["(3) 5.4節 (c) で紹介した方法は「重畳加算法（Overlap-add method)」と呼ばれる。同じ目的を達成する別の方法として「重畳保留法 (Overlap-save method)」がある。この方法について調べ，重畳加算法に対するメリットを述べなさい。\n","\n","【答】重畳保留法を簡単に説明する。$h(n)$は$N$点とし，その後に$N$点の0を付加して$\\hat h(n)$とするのは重畳加算法と同じである。重畳保留法では，入力$x(n)$を2$N$の長さでバッファする（最初の$N$点に関しては，その先頭に$N$個の0を付加することで2$N$点とする）。そして，2$N$点同士のFFTの積を逆FFTして時間波形に戻す。その前半$N$点は捨て，後半$N$点のみを出力とする方法である。オーバーラップの計算を省けるだけ，重畳加算法よりも計算コストは低いというメリットがある（本書では，「説明が容易そう」という理由で，重畳加算法を紹介した）。"]}]}